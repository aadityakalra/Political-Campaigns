{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Textblob\n",
    "2. Sentiword 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sen\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.collocations import *\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import show, figure\n",
    "from textblob import TextBlob\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Week1_Q1.txt', 'Week1_Q2.txt', 'Week1_Q3.txt', 'Week1_Q4.txt', 'Week2_Q1.txt', 'Week2_Q2.txt', 'Week2_Q3.txt']\n"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/Sen/Downloads/CUS 635 (Web data mining)/Project/Data/Combined/\"\n",
    "prefix = os.listdir(path)\n",
    "print(prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normalization\n",
    "\n",
    "def remove_utf(text):\n",
    "    return re.sub(r'[^\\x00-\\x7f]',r' ',text)\n",
    "\n",
    "def remove_punctuation(corpus):\n",
    "    punctuations = \".,\\\"-\\\\/#!?$%\\^&\\*;:{}=\\-_'~()\"    \n",
    "    filtered_corpus = [token for token in corpus if (not token in punctuations)]\n",
    "    return filtered_corpus\n",
    "\n",
    "def apply_stopwording(corpus, min_len):\n",
    "    filtered_corpus = [token for token in corpus if (not token in stopwords.words('english') and len(token)>min_len)]\n",
    "    return filtered_corpus\n",
    "\n",
    "def removeAbb(x):\n",
    "    lst = {'Dx':'diagnosed' ,\n",
    "           'Rx':'prescription',\n",
    "           'OTC':'Over The Counter',\n",
    "           'DFL':'Drug Fact Label',\n",
    "           'AUT':'Application Under Test'}\n",
    "    for i in x:\n",
    "        if i in lst:\n",
    "            i = lst[i]\n",
    "    return x\n",
    "\n",
    "def apply_lemmatization(corpus):\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    normalized_corpus = [lemmatizer.lemmatize(token) for token in corpus]\n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the dataset\n",
    "dataset={} #nltk text from tokens\n",
    "dataset_raw = {} \n",
    "allFeatures=set()\n",
    "tot_articles = 0\n",
    "articles_count={}\n",
    "raw_corpus = {} #used in sumerization\n",
    "dataset2= set()\n",
    "dataset3=[]\n",
    "N={} # Number of articles in each corpus\n",
    "\n",
    "for i,_ in enumerate(prefix):\n",
    "    fileName = path + prefix[i]\n",
    "    f=open(fileName,'r',encoding=\"utf8\")\n",
    "    text = ''\n",
    "    text_raw = '' \n",
    "    \n",
    "    lines = f.readlines()\n",
    "    #print(_,'OK') #load test\n",
    "    tot_articles+=len(lines)\n",
    "    articles_count[str(_)] = len(lines)\n",
    "    dataset_raw[str(_)] = list(map(lambda line: line.lower(), lines))\n",
    "\n",
    "    for line in lines:\n",
    "        dataset2.add(line.lower())\n",
    "        dataset3.append(remove_utf(line.lower()))\n",
    "        text+=line.replace('\\n',' ').lower()\n",
    "        text_raw = line.lower()\n",
    "    f.close\n",
    "    N[str(_)]=len(lines)\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    dataset[str(_)] = nltk.Text(tokens)\n",
    "    raw_corpus[_] = text\n",
    "\n",
    "    #Preprocessing\n",
    "dataset_clean={} #dict of tokens\n",
    "\n",
    "for i in dataset:\n",
    "    #print ('Processing %s' % str(i))\n",
    "    dataset_clean[i] = apply_lemmatization(removeAbb(apply_stopwording(remove_punctuation(dataset[i]), 3)))\n",
    "    #print (dataset_clean[i])\n",
    "\n",
    "\n",
    "#create a nltk.Text dict with clened dataset\n",
    "clean_text = {}\n",
    "for i in dataset_clean:\n",
    "    clean_text[i] = nltk.Text(dataset_clean[i])\n",
    "\n",
    "clean_text2 = \"\"\n",
    "for i in raw_corpus:\n",
    "    clean_text2 = raw_corpus[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subjectivity of the discussion topics for each question was generated through TextBlob. Subjectivity refers to being influenced by private opinions and beliefs, while objectivity refers to measurable facts generally agreed upon societies; 0 is objective and 1 is subjective. Product. Table below shows the key concepts for each question and their corresponding sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week1_Q1.txt -> Sentiment(polarity=0.13359126984126984, subjectivity=0.45107967032967033)\n",
      "Week1_Q2.txt -> Sentiment(polarity=0.1357051622212912, subjectivity=0.439565907807843)\n",
      "Week1_Q3.txt -> Sentiment(polarity=0.05862391774891774, subjectivity=0.4658237734487735)\n",
      "Week1_Q4.txt -> Sentiment(polarity=0.1295980669495521, subjectivity=0.47604314002828835)\n",
      "Week2_Q1.txt -> Sentiment(polarity=0.0981788326615913, subjectivity=0.4689543215405286)\n",
      "Week2_Q2.txt -> Sentiment(polarity=0.10206408267248969, subjectivity=0.469394930290948)\n",
      "Week2_Q3.txt -> Sentiment(polarity=0.1064038961038961, subjectivity=0.5041256854256854)\n"
     ]
    }
   ],
   "source": [
    "#textblobdf = pd.DataFrame()\n",
    "for i in prefix: \n",
    "    tblog = TextBlob(str(dataset_raw[i]))\n",
    "    #print(i)\n",
    "    print (i,\"->\",tblog.sentiment)\n",
    "    #textblobdf[i] = tblog.sentiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentiWordNet3.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis was conducted using the SentiWordNet3.0 Lexicon Dictionary for each week and question. The original lexicon was manually enhanced to better fit the specificity of the medical dataset by manipulating certain words’ scores. For instance, ‘risk’ and ‘cancer’ in all contexts were changed to negative 0.9, while words that may suggest benefits of the treatment were assigned more positive scores. The final score is a weighted average for all words in each question; with score above 1.5 sentiment is positive, score below 1.5 is negative, and scores in between indicate a neutral attitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = 'C:/Users/ziadm/Downloads/Web Data mining/'\n",
    "path2 = \"C:/Users/Sen/Downloads/CUS 635 (Web data mining)/Project/\"\n",
    "def loadSentiWordNet(lfile):\n",
    "    lf = open(lfile)\n",
    "    lines=lf.readlines()\n",
    "    lf.close\n",
    "    lexicon = {}\n",
    "    for line in lines:\n",
    "        info = line.split(\"\\t\")\n",
    "        try:\n",
    "            p_score = float(info[2])\n",
    "            n_score = float(info[3]) * -1.0\n",
    "            words = info[4].split(\" \")\n",
    "            for word in words:\n",
    "                if \"#\" in word:\n",
    "                    term = word.split(\"#\")\n",
    "                    lexicon[term[0]]= p_score + n_score\n",
    "        except:\n",
    "            pass\n",
    "    return lexicon\n",
    "\n",
    "lexicon_dictionary = \"newsenti.txt\"\n",
    "lex_fileName=path2+lexicon_dictionary\n",
    "lexicon_dictionary = loadSentiWordNet(lex_fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing=[]\n",
    "tokensnew = nltk.word_tokenize(clean_text2)\n",
    "cleaned_tokens2 = apply_stopwording(tokensnew,3)\n",
    "testing.append(cleaned_tokens2)\n",
    "'''\n",
    "for i in testing:\n",
    "    print(len(i))\n",
    "    for x in range(len(i)):\n",
    "        print(x)\n",
    "'''\n",
    "keys = lexicon_dictionary.keys()\n",
    "#print(lexicon_dictionary[\"concern\"])\n",
    "score = 0.0\n",
    "keys = lexicon_dictionary.keys()\n",
    "for i in testing:\n",
    "    for word in range(len(i)):\n",
    "        if i[word] in keys:\n",
    "            score = score + lexicon_dictionary[i[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Negative', -3.775)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scoreSentiment(testing,lex_dic):\n",
    "    score = 0.0\n",
    "    sentiment = \"Neutral\"\n",
    "    keys = lex_dic.keys()\n",
    "    for i in testing:\n",
    "        for word in range(len(i)):\n",
    "            if i[word] in keys:\n",
    "                #print(i[word],lexicon_dictionary[i[word]])\n",
    "                score = score + lexicon_dictionary[i[word]]   \n",
    "    if score >1.5:\n",
    "        sentiment = \"Positive\"\n",
    "    elif score<-1.5:\n",
    "        sentiment = \"Negative\"\n",
    "    return(sentiment, score) \n",
    "\n",
    "scoreSentiment(testing,lexicon_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiScore = pd.DataFrame()\n",
    "for i in prefix: \n",
    "    dataset_raw[i]\n",
    "    testing2=[]\n",
    "    #tokensnew2 = nltk.word_tokenize(str(dataset_raw[i]))\n",
    "    #cleaned_tokens3 = apply_stopwording(nltk.word_tokenize(str(dataset_raw[i])),3)\n",
    "    testing2.append(apply_stopwording(nltk.word_tokenize(str(dataset_raw[i])),3))\n",
    "    sentiScore[i] = scoreSentiment(testing2,lexicon_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Week1_Q1.txt</th>\n",
       "      <th>Week1_Q2.txt</th>\n",
       "      <th>Week1_Q3.txt</th>\n",
       "      <th>Week1_Q4.txt</th>\n",
       "      <th>Week2_Q1.txt</th>\n",
       "      <th>Week2_Q2.txt</th>\n",
       "      <th>Week2_Q3.txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.725</td>\n",
       "      <td>-18.181</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.825</td>\n",
       "      <td>-8.4</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-3.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Week1_Q1.txt Week1_Q2.txt Week1_Q3.txt Week1_Q4.txt Week2_Q1.txt  \\\n",
       "0     Positive     Negative     Positive      Neutral     Negative   \n",
       "1        9.725      -18.181          6.2        0.825         -8.4   \n",
       "\n",
       "  Week2_Q2.txt Week2_Q3.txt  \n",
       "0      Neutral     Negative  \n",
       "1         0.05        -3.65  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
